{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tempest Extreme experiment with NextGEMS data\n",
    "Data have been already preprocessed to 1x1 grid on Levante\n",
    "\n",
    "We start by reading and putting together into a single netcdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "import subprocess\n",
    "from time import time\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "def readwrite_from_lowres(filein, fileout) : \n",
    "\n",
    "    \"\"\"\n",
    "    Read and write low resolution data to mimic access from FDB\n",
    "\n",
    "    Args: \n",
    "        filein: input file at low resolution\n",
    "        fileout: input file at low resolution (netcdf)\n",
    "\n",
    "    Returns: \n",
    "        outdict: dictionary with variable and dimensiona names for fileout\n",
    "    \"\"\"\n",
    "\n",
    "    xfield = xr.open_mfdataset(filein)\n",
    "\n",
    "    # check if output file exists\n",
    "    if os.path.exists(fileout):\n",
    "        os.remove(fileout)\n",
    "\n",
    "    xfield.to_netcdf(fileout)\n",
    "    xfield.close()\n",
    "    \n",
    "    outdict = {'lon': 'lon', 'lat': 'lat', \n",
    "            'psl': 'MSL', 'zg': 'Z',\n",
    "            'uas': 'U10M', 'vas': 'V10M'}\n",
    "\n",
    "    return outdict\n",
    "\n",
    "\n",
    "def run_detect_nodes(tempest_dictionary, tempest_filein, tempest_fileout) : \n",
    "\n",
    "    \"\"\"\"\n",
    "    Basic function to call from command line tempest extremes DetectNodes\n",
    "    \"\"\"\n",
    "    \n",
    "    detect_string= f'DetectNodes --in_data {tempest_filein} --timefilter 6hr --out {tempest_fileout} --searchbymin {tempest_dictionary[\"psl\"]} ' \\\n",
    "    f'--closedcontourcmd {tempest_dictionary[\"psl\"]},200.0,5.5,0;_DIFF({tempest_dictionary[\"zg\"]}(30000Pa),{tempest_dictionary[\"zg\"]}(50000Pa)),-58.8,6.5,1.0 --mergedist 6.0 ' \\\n",
    "    f'--outputcmd {tempest_dictionary[\"psl\"]},min,0;_VECMAG({tempest_dictionary[\"uas\"]},{tempest_dictionary[\"vas\"]}),max,2 --latname {tempest_dictionary[\"lat\"]} --lonname {tempest_dictionary[\"lon\"]}'\n",
    "\n",
    "    subprocess.run(detect_string.split(), stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "\n",
    "    return detect_string\n",
    "\n",
    "def read_lonlat_nodes_old(tempest_fileout) :\n",
    "\n",
    "    \"\"\"\n",
    "    Read from txt files output of DetectNodes the position of the centers of the TCs\n",
    "    \"\"\"\n",
    "\n",
    "    # open the output file and extract the required lon/lat\n",
    "    with open(tempest_fileout) as f:\n",
    "       data = f.readlines()\n",
    "    first = data[0].split('\\t')\n",
    "    date = first[0] + first[1].zfill(2) + first[2].zfill(2) + first[4].rstrip().zfill(2)\n",
    "    slon =[]\n",
    "    slat = []\n",
    "    for line in data[1:] : \n",
    "        slon = slon + [line.split('\\t')[3]]\n",
    "        slat = slat + [line.split('\\t')[4]]\n",
    "    out = {'date': date, 'lon' : slon, 'lat' : slat}\n",
    "    return out\n",
    "\n",
    "def read_lonlat_nodes(tempest_fileout):\n",
    "    \"\"\"\n",
    "    Read from txt files output of DetectNodes the position of the centers of the TCs\n",
    "    \"\"\"\n",
    "    with open(tempest_fileout) as f:\n",
    "        lines = f.readlines()\n",
    "    first = lines[0].split('\\t')\n",
    "    date = first[0] + first[1].zfill(2) + first[2].zfill(2) + first[4].rstrip().zfill(2)\n",
    "    lon_lat = [line.split('\\t')[3:] for line in lines[1:]]\n",
    "    out = {'date': date, 'lon': [val[0] for val in lon_lat], 'lat': [val[1] for val in lon_lat]}\n",
    "    return out\n",
    "\n",
    "\n",
    "def lonlatbox(lon, lat, delta) : \n",
    "    \"\"\"\n",
    "    Define the list for the box\n",
    "    \"\"\"\n",
    "    return [float(lon) - delta, float(lon) +delta, float(lat) -delta, float(lat) + delta]\n",
    "\n",
    "def store_fullres_field(mfield, xfield, nodes, boxdim = 10): \n",
    "\n",
    "    \"\"\"Create xarray element that keep only the values of a field around the TC nodes\"\"\"\n",
    "\n",
    "    mask = xfield * 0\n",
    "    for k in range(0, len(nodes['lon'])) :\n",
    "        # add safe condition: keep only data between 50S and 50N\n",
    "        #if (float(nodes['lat'][k]) > -50) and (float(nodes['lat'][k]) < 50): \n",
    "        box = lonlatbox(nodes['lon'][k], nodes['lat'][k], boxdim)\n",
    "        mask = mask + xr.where((xfield.lon > box[0]) & (xfield.lon < box[1]) & (xfield.lat > box[2]) & (xfield.lat < box[3]), True, False)\n",
    "\n",
    "    outfield = xfield.where(mask>0)\n",
    "\n",
    "    if isinstance(mfield, xr.DataArray):\n",
    "        outfield = xr.concat([mfield, outfield], dim = 'time')\n",
    "    \n",
    "    return outfield\n",
    "\n",
    "def write_fullres_field(gfield, filestore): \n",
    "\n",
    "    #compression = {'MSL': {'zlib': True}}\n",
    "    compression = {'MSL': {'zlib': True, 'complevel': 1}}\n",
    "    gfield.where(gfield!=0).to_netcdf(filestore, encoding=compression)\n",
    "    gfield.close()\n",
    "\n",
    "def clean_files(filelist):\n",
    "\n",
    "    for fileout in filelist :\n",
    "        if os.path.exists(fileout):\n",
    "            os.remove(fileout)\n",
    "\n",
    "def run_stitch_nodes(infiles_list, trackfile):\n",
    "\n",
    "    full_nodes = os.path.join('full_nodes.txt')\n",
    "    if os.path.exists(full_nodes):\n",
    "            os.remove(full_nodes)\n",
    "\n",
    "    with open(full_nodes, 'w') as outfile:\n",
    "        for fname in sorted(infiles_list):\n",
    "            with open(fname) as infile:\n",
    "                outfile.write(infile.read())\n",
    "\n",
    "    \n",
    "    stitch_string = f'StitchNodes --in {full_nodes} --out {trackfile} --in_fmt lon,lat,slp,wind --range 8.0 --mintime 54h ' \\\n",
    "        f'--maxgap 24h --threshold wind,>=,10.0,10;lat,<=,50.0,10;lat,>=,-50.0,10'\n",
    "    subprocess.run(stitch_string.split())\n",
    "    return stitch_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pa# input directory\n",
    "indir='/home/b/b382216/scratch/regrid'\n",
    "tmpdir='/home/b/b382216/scratch/tmpdir'\n",
    "fulldir='/home/b/b382216/scratch/fullres'\n",
    "\n",
    "time = '000120'\n",
    "infile = os.path.join(indir, f'regrid+{time}_*.nc')\n",
    "outfile = os.path.join(tmpdir, time + '.nc')\n",
    "txtfile = os.path.join(tmpdir, 'output_' + time + '.txt')\n",
    "mslfile=os.path.join(fulldir, f'ICMGGhqys+{time}_msl.nc')\n",
    "storefile = os.path.join(tmpdir, 'TC_' + time + '.nc')\n",
    "\n",
    "# read and save the netcdf file to mimic the time lost by the FDB query\n",
    "tempest_dictionary = readwrite_from_lowres(infile, outfile)\n",
    "tempest_command = run_detect_nodes(tempest_dictionary, outfile, txtfile)\n",
    "tempest_nodes = read_lonlat_nodes(txtfile) \n",
    "\n",
    "xfield = xr.open_mfdataset(outfile)['MSL']\n",
    "\n",
    "gfield = xfield * 0\n",
    "for k in range(0, len(tempest_nodes['lon'])) :\n",
    "    box = lonlatbox(tempest_nodes['lon'][k], tempest_nodes['lat'][k], 10)\n",
    "    gfield = gfield + xr.where((xfield.lon > box[0]) & (xfield.lon < box[1]) & (xfield.lat > box[2]) & (xfield.lat < box[3]), True, False)\n",
    "\n",
    "xfield = xfield.where(gfield>0)\n",
    "\n",
    "xfield.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "006000\n",
      "Timestep done in 0.5145 seconds\n",
      "2020020112\n",
      "Storing output\n",
      "Storing timestep data done in 0.9771 seconds\n",
      "006120\n",
      "Timestep done in 0.4299 seconds\n",
      "2020020118\n",
      "Storing output\n",
      "Storing timestep data done in 1.2800 seconds\n",
      "006240\n",
      "Timestep done in 0.2842 seconds\n",
      "2020020200\n",
      "Storing output\n",
      "Storing timestep data done in 1.0040 seconds\n",
      "006360\n",
      "Timestep done in 0.5731 seconds\n",
      "2020020206\n",
      "Storing output\n",
      "Storing timestep data done in 1.1940 seconds\n",
      "006480\n",
      "Timestep done in 0.3484 seconds\n",
      "2020020212\n",
      "Storing output\n",
      "Storing timestep data done in 1.0531 seconds\n",
      "006600\n",
      "Timestep done in 0.7050 seconds\n",
      "2020020218\n",
      "Storing output\n",
      "Storing timestep data done in 1.5299 seconds\n",
      "006720\n",
      "Timestep done in 0.4912 seconds\n",
      "2020020300\n",
      "Storing output\n",
      "Storing timestep data done in 1.2131 seconds\n",
      "006840\n",
      "Timestep done in 0.5674 seconds\n",
      "2020020306\n",
      "Storing output\n",
      "Storing timestep data done in 1.3032 seconds\n",
      "006960\n",
      "Timestep done in 0.6927 seconds\n",
      "2020020312\n",
      "Storing output\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: b'/home/b/b382216/scratch/tmpdir/TC_MSL_2020020312.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/work/bb1153/b382216/mambaforge/envs/TCs/lib/python3.10/site-packages/xarray/backends/file_manager.py:209\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cache[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_key]\n\u001b[1;32m    210\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m/work/bb1153/b382216/mambaforge/envs/TCs/lib/python3.10/site-packages/xarray/backends/lru_cache.py:55\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m---> 55\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cache[key]\n\u001b[1;32m     56\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache\u001b[39m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/home/b/b382216/scratch/tmpdir/TC_MSL_2020020312.nc',), 'a', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), 'bfb7ba51-188b-4433-b7e0-e130aac4bb19']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m# store the file\u001b[39;00m\n\u001b[1;32m     44\u001b[0m store_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(tmpdir, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTC_MSL_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mid\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.nc\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m write_fullres_field(xfield, store_file)\n\u001b[1;32m     47\u001b[0m \u001b[39m##reinit xfield\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m#xfield = 0\u001b[39;00m\n\u001b[1;32m     50\u001b[0m toc \u001b[39m=\u001b[39m time()\n",
      "Cell \u001b[0;32mIn[20], line 98\u001b[0m, in \u001b[0;36mwrite_fullres_field\u001b[0;34m(gfield, filestore)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite_fullres_field\u001b[39m(gfield, filestore): \n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m     \u001b[39m#compression = {'MSL': {'zlib': True}}\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     compression \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mMSL\u001b[39m\u001b[39m'\u001b[39m: {\u001b[39m'\u001b[39m\u001b[39mzlib\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mTrue\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mcomplevel\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m}}\n\u001b[0;32m---> 98\u001b[0m     gfield\u001b[39m.\u001b[39;49mwhere(gfield\u001b[39m!=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mto_netcdf(filestore, encoding\u001b[39m=\u001b[39;49mcompression)\n\u001b[1;32m     99\u001b[0m     gfield\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/work/bb1153/b382216/mambaforge/envs/TCs/lib/python3.10/site-packages/xarray/core/dataarray.py:3964\u001b[0m, in \u001b[0;36mDataArray.to_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[1;32m   3960\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3961\u001b[0m     \u001b[39m# No problems with the name - so we're fine!\u001b[39;00m\n\u001b[1;32m   3962\u001b[0m     dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_dataset()\n\u001b[0;32m-> 3964\u001b[0m \u001b[39mreturn\u001b[39;00m to_netcdf(  \u001b[39m# type: ignore  # mypy cannot resolve the overloads:(\u001b[39;49;00m\n\u001b[1;32m   3965\u001b[0m     dataset,\n\u001b[1;32m   3966\u001b[0m     path,\n\u001b[1;32m   3967\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   3968\u001b[0m     \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mformat\u001b[39;49m,\n\u001b[1;32m   3969\u001b[0m     group\u001b[39m=\u001b[39;49mgroup,\n\u001b[1;32m   3970\u001b[0m     engine\u001b[39m=\u001b[39;49mengine,\n\u001b[1;32m   3971\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   3972\u001b[0m     unlimited_dims\u001b[39m=\u001b[39;49munlimited_dims,\n\u001b[1;32m   3973\u001b[0m     compute\u001b[39m=\u001b[39;49mcompute,\n\u001b[1;32m   3974\u001b[0m     multifile\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   3975\u001b[0m     invalid_netcdf\u001b[39m=\u001b[39;49minvalid_netcdf,\n\u001b[1;32m   3976\u001b[0m )\n",
      "File \u001b[0;32m/work/bb1153/b382216/mambaforge/envs/TCs/lib/python3.10/site-packages/xarray/backends/api.py:1215\u001b[0m, in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1211\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1212\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1213\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munrecognized option \u001b[39m\u001b[39m'\u001b[39m\u001b[39minvalid_netcdf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for engine \u001b[39m\u001b[39m{\u001b[39;00mengine\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m         )\n\u001b[0;32m-> 1215\u001b[0m store \u001b[39m=\u001b[39m store_open(target, mode, \u001b[39mformat\u001b[39;49m, group, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1217\u001b[0m \u001b[39mif\u001b[39;00m unlimited_dims \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m     unlimited_dims \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mencoding\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39munlimited_dims\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/work/bb1153/b382216/mambaforge/envs/TCs/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:382\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    376\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    377\u001b[0m     clobber\u001b[39m=\u001b[39mclobber, diskless\u001b[39m=\u001b[39mdiskless, persist\u001b[39m=\u001b[39mpersist, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mformat\u001b[39m\n\u001b[1;32m    378\u001b[0m )\n\u001b[1;32m    379\u001b[0m manager \u001b[39m=\u001b[39m CachingFileManager(\n\u001b[1;32m    380\u001b[0m     netCDF4\u001b[39m.\u001b[39mDataset, filename, mode\u001b[39m=\u001b[39mmode, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    381\u001b[0m )\n\u001b[0;32m--> 382\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(manager, group\u001b[39m=\u001b[39;49mgroup, mode\u001b[39m=\u001b[39;49mmode, lock\u001b[39m=\u001b[39;49mlock, autoclose\u001b[39m=\u001b[39;49mautoclose)\n",
      "File \u001b[0;32m/work/bb1153/b382216/mambaforge/envs/TCs/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:329\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_group \u001b[39m=\u001b[39m group\n\u001b[1;32m    328\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode \u001b[39m=\u001b[39m mode\n\u001b[0;32m--> 329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mds\u001b[39m.\u001b[39mdata_model\n\u001b[1;32m    330\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filename \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mds\u001b[39m.\u001b[39mfilepath()\n\u001b[1;32m    331\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_remote \u001b[39m=\u001b[39m is_remote_uri(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filename)\n",
      "File \u001b[0;32m/work/bb1153/b382216/mambaforge/envs/TCs/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:391\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mds\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 391\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_acquire()\n",
      "File \u001b[0;32m/work/bb1153/b382216/mambaforge/envs/TCs/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:385\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_acquire\u001b[39m(\u001b[39mself\u001b[39m, needs_lock\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 385\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_manager\u001b[39m.\u001b[39macquire_context(needs_lock) \u001b[39mas\u001b[39;00m root:\n\u001b[1;32m    386\u001b[0m         ds \u001b[39m=\u001b[39m _nc4_require_group(root, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_group, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode)\n\u001b[1;32m    387\u001b[0m     \u001b[39mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/work/bb1153/b382216/mambaforge/envs/TCs/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    136\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m/work/bb1153/b382216/mambaforge/envs/TCs/lib/python3.10/site-packages/xarray/backends/file_manager.py:197\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39m@contextlib\u001b[39m\u001b[39m.\u001b[39mcontextmanager\n\u001b[1;32m    195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39macquire_context\u001b[39m(\u001b[39mself\u001b[39m, needs_lock\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    196\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     file, cached \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_acquire_with_cache_info(needs_lock)\n\u001b[1;32m    198\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m         \u001b[39myield\u001b[39;00m file\n",
      "File \u001b[0;32m/work/bb1153/b382216/mambaforge/envs/TCs/lib/python3.10/site-packages/xarray/backends/file_manager.py:215\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    213\u001b[0m     kwargs \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    214\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode\n\u001b[0;32m--> 215\u001b[0m file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_opener(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    217\u001b[0m     \u001b[39m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2463\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2026\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: b'/home/b/b382216/scratch/tmpdir/TC_MSL_2020020312.nc'"
     ]
    }
   ],
   "source": [
    "\n",
    "# input directory\n",
    "indir='/home/b/b382216/scratch/regrid'\n",
    "tmpdir='/home/b/b382216/scratch/tmpdir'\n",
    "fulldir='/home/b/b382216/scratch/fullres'\n",
    "#dask.config.set(scheduler=\"synchronous\")\n",
    "\n",
    "# loop on timerecors\n",
    "for t in range(6000, 12000, 120) : \n",
    "    tic = time()\n",
    "\n",
    "    tttt = str(t).zfill(6)\n",
    "    print(tttt)\n",
    "    # path definition\n",
    "    original_file = os.path.join(indir, f'regrid+{tttt}_*.nc')\n",
    "    netcdf_file = os.path.join(tmpdir, tttt + '.nc')\n",
    "    txt_file = os.path.join(tmpdir, 'output_' + tttt + '.txt')\n",
    "    msl_fullres_file=os.path.join(fulldir, f'ICMGGhqys+{tttt}_msl.nc')\n",
    "   \n",
    "\n",
    "    # read and save the netcdf file to mimic the time lost by the FDB query\n",
    "    tempest_dictionary = readwrite_from_lowres(original_file, netcdf_file)\n",
    "\n",
    "    # run tempest extremes\n",
    "    tempest_command = run_detect_nodes(tempest_dictionary, netcdf_file, txt_file)\n",
    "    tempest_nodes = read_lonlat_nodes(txt_file)\n",
    "\n",
    "    # get the full res field and store the required values around the Nodes\n",
    "    fullres_field = xr.open_mfdataset(msl_fullres_file)['MSL']\n",
    "    xfield = store_fullres_field(0, fullres_field, tempest_nodes)\n",
    "    clean_files([netcdf_file])\n",
    "    \n",
    "    toc = time()\n",
    "    print('Timestep done in {:.4f} seconds'.format(toc - tic))\n",
    "\n",
    "    # store data every day\n",
    "    #if (t+120) % (120*4) == 0 : \n",
    "        #day = str((t+120) // (120*4)).zfill(3)\n",
    "    id = tempest_nodes['date']\n",
    "    print(id)\n",
    "    print('Storing output')\n",
    "    tic = time()\n",
    "\n",
    "    # store the file\n",
    "    store_file = os.path.join(tmpdir, f'TC_MSL_{id}.nc')\n",
    "    write_fullres_field(xfield, store_file)\n",
    "\n",
    "    ##reinit xfield\n",
    "    #xfield = 0\n",
    "\n",
    "    toc = time()\n",
    "    print('Storing timestep data done in {:.4f} seconds'.format(toc - tic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m track_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(tmpdir, \u001b[39m'\u001b[39m\u001b[39mtrack.txt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m filenames \u001b[39m=\u001b[39m glob(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(tmpdir,\u001b[39m'\u001b[39;49m\u001b[39moutput*.txt\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m      3\u001b[0m stitch_string \u001b[39m=\u001b[39m run_stitch_nodes(filenames, track_file)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "track_file = os.path.join(tmpdir, 'track.txt')\n",
    "filenames = glob(os.path.join(tmpdir,'output*.txt'))\n",
    "stitch_string = run_stitch_nodes(filenames, track_file)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the lon/lat from the txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StitchNodes --in /home/b/b382216/scratch/tmpdir/full_nodes.txt --out /home/b/b382216/scratch/tmpdir/track.txt --in_fmt lon,lat,slp,wind --range 8.0 --mintime 54h --maxgap 24h --threshold wind,>=,10.0,10;lat,<=,50.0,10;lat,>=,-50.0,10\n",
      "Arguments:\n",
      "  --in <string> [\"/home/b/b382216/scratch/tmpdir/full_nodes.txt\"] \n",
      "  --in_list <string> [\"\"] \n",
      "  --in_connect <string> [\"\"] \n",
      "  --out <string> [\"/home/b/b382216/scratch/tmpdir/track.txt\"] \n",
      "  --in_fmt <string> [\"lon,lat,slp,wind\"] \n",
      "  --range <double> [8.000000] (degrees)\n",
      "  --mintime <string> [\"54h\"] \n",
      "  --time_begin <string> [\"\"] \n",
      "  --time_end <string> [\"\"] \n",
      "  --min_endpoint_dist <double> [0.000000] (degrees)\n",
      "  --min_path_dist <double> [0.000000] (degrees)\n",
      "  --maxgap <string> [\"24h\"] \n",
      "  --threshold <string> [\"wind,>=,10.0,10;lat,<=,50.0,10;lat,>=,-50.0,10\"] [col,op,value,count;...]\n",
      "  --caltype <string> [\"standard\"] (none|standard|noleap|360_day)\n",
      "  --allow_repeated_times <bool> [false] \n",
      "  --out_file_format <string> [\"gfdl\"] (gfdl|csv|csvnohead)\n",
      "------------------------------------------------------------\n",
      "Parsing thresholds\n",
      "..wind greater than or equal to 10.000000 at least 10 time(s)\n",
      "..lat less than or equal to 50.000000 at least 10 time(s)\n",
      "..lat greater than or equal to -50.000000 at least 10 time(s)\n",
      "..Done\n",
      "Loading candidate data\n",
      "..Discrete times: 50 (2020-02-01 12:00:00 to 2020-02-13 18:00:00)\n",
      "..Done\n",
      "Creating KD trees at each time level.. Done\n",
      "Populating set of path segments.. Done\n",
      "Constructing paths\n",
      "..Paths rejected (mintime): 36\n",
      "..Paths rejected (minendpointdist): 0\n",
      "..Paths rejected (minpathdist): 0\n",
      "..Paths rejected (threshold): 3\n",
      "..Total paths found: 2\n",
      "..Done\n",
      "Writing results.. Done\n",
      "Cleanup.. Done\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['StitchNodes', '--in', '/home/b/b382216/scratch/tmpdir/full_nodes.txt', '--out', '/home/b/b382216/scratch/tmpdir/track.txt', '--in_fmt', 'lon,lat,slp,wind', '--range', '8.0', '--mintime', '54h', '--maxgap', '24h', '--threshold', 'wind,>=,10.0,10;lat,<=,50.0,10;lat,>=,-50.0,10'], returncode=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020020312\n",
      "2020020318\n",
      "2020020400\n",
      "2020020406\n",
      "2020020412\n",
      "2020020418\n",
      "2020020500\n",
      "2020020506\n",
      "2020020512\n",
      "2020020518\n",
      "2020020600\n",
      "2020020606\n",
      "2020020612\n",
      "2020020618\n",
      "2020020700\n",
      "2020020706\n",
      "2020020712\n",
      "2020020718\n",
      "2020020800\n",
      "2020020806\n",
      "2020020812\n",
      "2020020818\n",
      "2020020900\n",
      "2020020906\n",
      "2020020912\n",
      "2020020918\n",
      "2020021000\n",
      "2020021006\n",
      "2020021012\n",
      "2020021018\n",
      "2020021100\n",
      "2020021106\n",
      "2020021112\n",
      "2020021118\n",
      "2020021200\n",
      "2020021206\n",
      "2020021212\n",
      "2020021218\n",
      "2020021300\n",
      "2020021306\n",
      "2020021312\n",
      "2020021318\n",
      "Storing output\n",
      "Storing timestep data done in 133.7639 seconds\n"
     ]
    }
   ],
   "source": [
    " # open the output file and extract the required lon/lat\n",
    "track_file = os.path.join(tmpdir, 'track.txt')\n",
    "\n",
    "\n",
    "with open(track_file) as file:\n",
    "    lines = file.read().splitlines()\n",
    "    parts_list = [line.split(\"\\t\") for line in lines if len(line.split(\"\\t\")) > 6]\n",
    "    #print(parts_list)\n",
    "    tracks ={'slon': [parts[3] for parts in parts_list],\n",
    "        'slat':  [parts[4] for parts in parts_list],\n",
    "        'date': [parts[7] + parts[8].zfill(2) + parts[9].zfill(2) + parts[10].zfill(2) for parts in parts_list],\n",
    "    }\n",
    "\n",
    "reorder_track ={}\n",
    "for tstep in sorted(set(tracks['date'])) : \n",
    "    #idx = tracks['date'].index(tstep)\n",
    "    idx = [i for i, e in enumerate(tracks['date']) if e == tstep]\n",
    "    reorder_track[tstep] = {}\n",
    "    reorder_track[tstep]['lon'] = [tracks['slon'][k] for k in idx]\n",
    "    reorder_track[tstep]['lat'] = [tracks['slat'][k] for k in idx]\n",
    "\n",
    "\n",
    "\n",
    "xfield = 0\n",
    "for id in reorder_track.keys():\n",
    "\n",
    "    print(id)\n",
    "    fullres_file = os.path.join(tmpdir, f'TC_MSL_{id}.nc')\n",
    "    fullres_field = xr.open_mfdataset(fullres_file)['MSL']\n",
    "\n",
    "    # get the full res field and store the required values around the Nodes\n",
    "    #fullres_field = xr.open_mfdataset(msl_fullres_file)['MSL']\n",
    "    xfield = store_fullres_field(xfield, fullres_field, tempest_nodes)\n",
    "\n",
    "\n",
    "print('Storing output')\n",
    "tic = time()\n",
    "\n",
    "# store the file\n",
    "store_file = os.path.join(tmpdir, 'tempest_tracks.nc')\n",
    "write_fullres_field(xfield, store_file)\n",
    "toc = time()\n",
    "print('Storing timestep data done in {:.4f} seconds'.format(toc - tic))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this diagnostic to test the new aqua.docker.rundiag() function\n",
    "# Everything is defined in the diagnostic.yaml and machine.yaml files\n",
    "# If no argument is provided then the first command defined in machine.yaml is run\n",
    "#output = aqua.docker.rundiag(cmd=\"detect\")\n",
    "\n",
    "# replace the dokker structure with a python environment which does the following:\n",
    "\n",
    "# detect_out = aqua.docker.rundiag(cmd=\"detect\")\n",
    "# stitch_out = aqua.docker.rundiag(cmd=\"stitch\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from aqua import regrid\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import re\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from cartopy import config\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "#import tempest_lib as tmp\n",
    "\n",
    "\n",
    "diagname = 'TCs'\n",
    "machine = 'levante'\n",
    "\n",
    "\n",
    "with open(f'../../AQUA/diagnostics/config/config_{machine}.yml', 'r', encoding='utf-8') as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "with open(f'{diagname}.yml', 'r', encoding='utf-8') as file:\n",
    "    namelist = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "#regrid files from original res to 1x1\n",
    "\n",
    "from cdo import *   # python version\n",
    "cdo = Cdo()\n",
    "\n",
    "k_init=000000\n",
    "k_final=166560\n",
    "#loop to regrid files \n",
    "for k in range(k_init,k_final):\n",
    "\n",
    "        print(\"GH regridding\")\n",
    "        cdo genbil,r360x180 file_in weights.nc\n",
    "        cdo remap, r360x180, weights file_in file_out\n",
    "\n",
    "        print(\"lnsp regridding\")\n",
    "        print(\"10u regridding\")\n",
    "        print(\"10v regridding\")\n",
    "\n",
    "\n",
    "# ADJUST TEMPEST COMMANDS IN THE RESPECTIVE YML FILES\n",
    "\n",
    "# first run detect_nodes to detect the TCs centers\n",
    "subprocess.run(['bash', './detect_nodes'])\n",
    "\n",
    "# then compute trajectories using stitch_nodes\n",
    "subprocess.run(['bash', './stitch_nodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/zarzycki/cymep\n",
    "\n",
    "def getTrajectories(filename,nVars,headerDelimStr,isUnstruc):\n",
    "  print(\"Getting trajectories from TempestExtremes file...\")\n",
    "  print(\"Running getTrajectories on '%s' with unstruc set to '%s'\" % (filename, isUnstruc))\n",
    "  print(\"nVars set to %d and headerDelimStr set to '%s'\" % (nVars, headerDelimStr))\n",
    "\n",
    "  # Using the newer with construct to close the file automatically.\n",
    "  with open(filename) as f:\n",
    "      data = f.readlines()\n",
    "\n",
    "  # Find total number of trajectories and maximum length of trajectories\n",
    "  numtraj=0\n",
    "  numPts=[]\n",
    "  for line in data:\n",
    "    if headerDelimStr in line:\n",
    "      # if header line, store number of points in given traj in numPts\n",
    "      headArr = line.split()\n",
    "      numtraj += 1\n",
    "      numPts.append(int(headArr[1]))\n",
    "    else:\n",
    "      # if not a header line, and nVars = -1, find number of columns in data point\n",
    "      if nVars < 0:\n",
    "        nVars=len(line.split())\n",
    "  \n",
    "  maxNumPts = max(numPts) # Maximum length of ANY trajectory\n",
    "\n",
    "  print(\"Found %d columns\" % nVars)\n",
    "  print(\"Found %d trajectories\" % numtraj)\n",
    "\n",
    "  # Initialize storm and line counter\n",
    "  stormID=-1\n",
    "  lineOfTraj=-1\n",
    "\n",
    "  # Create array for data\n",
    "  if isUnstruc:\n",
    "    prodata = np.empty((nVars+1,numtraj,maxNumPts))\n",
    "  else:\n",
    "    prodata = np.empty((nVars,numtraj,maxNumPts))\n",
    "\n",
    "  prodata[:] = np.NAN\n",
    "\n",
    "  for i, line in enumerate(data):\n",
    "    if headerDelimStr in line:  # check if header string is satisfied\n",
    "      stormID += 1      # increment storm\n",
    "      lineOfTraj = 0    # reset trajectory line to zero\n",
    "    else:\n",
    "      ptArr = line.split()\n",
    "      for jj in range(nVars):\n",
    "        if isUnstruc:\n",
    "          prodata[jj+1,stormID,lineOfTraj]=ptArr[jj]\n",
    "        else:\n",
    "          prodata[jj,stormID,lineOfTraj]=ptArr[jj]\n",
    "      lineOfTraj += 1   # increment line\n",
    "\n",
    "  print(\"... done reading data\")\n",
    "  return numtraj, maxNumPts, prodata\n",
    "\n",
    "\n",
    "def getNodes(filename,nVars,isUnstruc):\n",
    "  print(\"Getting nodes from TempestExtremes file...\")\n",
    "\n",
    "  # Using the newer with construct to close the file automatically.\n",
    "  with open(filename) as f:\n",
    "      data = f.readlines()\n",
    "\n",
    "  # Find total number of trajectories and maximum length of trajectories\n",
    "  numnodetimes=0\n",
    "  numPts=[]\n",
    "  for line in data:\n",
    "    if re.match(r'\\w', line):\n",
    "      # if header line, store number of points in given traj in numPts\n",
    "      headArr = line.split()\n",
    "      numnodetimes += 1\n",
    "      numPts.append(int(headArr[3]))\n",
    "    else:\n",
    "      # if not a header line, and nVars = -1, find number of columns in data point\n",
    "      if nVars < 0:\n",
    "        nVars=len(line.split())\n",
    "  \n",
    "  maxNumPts = max(numPts) # Maximum length of ANY trajectory\n",
    "\n",
    "  print(\"Found %d columns\" % nVars)\n",
    "  print(\"Found %d trajectories\" % numnodetimes)\n",
    "  print(\"Found %d maxNumPts\" % maxNumPts)\n",
    "\n",
    "  # Initialize storm and line counter\n",
    "  stormID=-1\n",
    "  lineOfTraj=-1\n",
    "\n",
    "  # Create array for data\n",
    "  if isUnstruc:\n",
    "    prodata = np.empty((nVars+5,numnodetimes,maxNumPts))\n",
    "  else:\n",
    "    prodata = np.empty((nVars+4,numnodetimes,maxNumPts))\n",
    "\n",
    "  prodata[:] = np.NAN\n",
    "\n",
    "  nextHeadLine=0\n",
    "  for i, line in enumerate(data):\n",
    "    if re.match(r'\\w', line):  # check if header string is satisfied\n",
    "      stormID += 1      # increment storm\n",
    "      lineOfTraj = 0    # reset trajectory line to zero\n",
    "      headArr = line.split()\n",
    "      YYYY = int(headArr[0])\n",
    "      MM = int(headArr[1])\n",
    "      DD = int(headArr[2])\n",
    "      HH = int(headArr[4])\n",
    "    else:\n",
    "      ptArr = line.split()\n",
    "      for jj in range(nVars-1):\n",
    "        if isUnstruc:\n",
    "          prodata[jj+1,stormID,lineOfTraj]=ptArr[jj]\n",
    "        else:\n",
    "          prodata[jj,stormID,lineOfTraj]=ptArr[jj]\n",
    "      if isUnstruc:\n",
    "        prodata[nVars+1,stormID,lineOfTraj]=YYYY\n",
    "        prodata[nVars+2,stormID,lineOfTraj]=MM\n",
    "        prodata[nVars+3,stormID,lineOfTraj]=DD\n",
    "        prodata[nVars+4,stormID,lineOfTraj]=HH\n",
    "      else:\n",
    "        prodata[nVars  ,stormID,lineOfTraj]=YYYY\n",
    "        prodata[nVars+1,stormID,lineOfTraj]=MM\n",
    "        prodata[nVars+2,stormID,lineOfTraj]=DD\n",
    "        prodata[nVars+3,stormID,lineOfTraj]=HH\n",
    "      lineOfTraj += 1   # increment line\n",
    "\n",
    "  print(\"... done reading data\")\n",
    "  return numnodetimes, maxNumPts, prodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempest settings\n",
    "nVars=-1\n",
    "headerStr='start'\n",
    "isUnstruc = 0\n",
    "\n",
    "# Extract trajectories from tempest file and assign to arrays\n",
    "# USER_MODIFY\n",
    "nstorms, ntimes, traj_data = getTrajectories(trajfile,nVars,headerStr,isUnstruc)\n",
    "xlon   = traj_data[2,:,:]\n",
    "xlat   = traj_data[3,:,:]\n",
    "xpres  = traj_data[4,:,:]/100.\n",
    "xwind  = traj_data[5,:,:]\n",
    "xyear  = traj_data[7,:,:]\n",
    "xmonth = traj_data[8,:,:]\n",
    "\n",
    "# Initialize axes\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "# ax.set_extent([-180, 180, -75, 75], crs=None)\n",
    "\n",
    "# Set title and subtitle\n",
    "plt.title('Example of a Trajectory Plot')\n",
    "\n",
    "\n",
    "# Set land feature and change color to 'lightgrey'\n",
    "# See link for extensive list of colors:\n",
    "# https://matplotlib.org/3.1.0/gallery/color/named_colors.html\n",
    "ax.add_feature(cfeature.LAND, color='lightgrey')\n",
    "\n",
    "gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                  linewidth=0.5, color='k', alpha=0.5, linestyle='--')\n",
    "gl.xlabels_top = False\n",
    "gl.ylabels_left = False\n",
    "#gl.xlines = False\n",
    "gl.xlocator = mticker.FixedLocator([-180, -90, 0, 90, 180])\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.ylabel_style = {'size': 12, 'color': 'black'}\n",
    "gl.xlabel_style = {'size': 12, 'color': 'black'}\n",
    "\n",
    "\n",
    "\n",
    "# Plot each trajectory\n",
    "for i in range(nstorms):\n",
    "\n",
    "        # doesn't work with cartopy!\n",
    "        #plt.plot(xlon[i], xlat[i], linewidth=0.4)\n",
    "\n",
    "\n",
    "\n",
    "    plt.scatter(x=xlon[i], y=xlat[i],\n",
    "                                                color=\"black\",\n",
    "                                                s=30,\n",
    "                                                linewidths=0.5,\n",
    "                                                marker=\".\",\n",
    "                                                alpha=0.8,\n",
    "                                                transform=ccrs.PlateCarree()) ## Important\n",
    "\n",
    "\n",
    "plt.savefig(plotdir + \"prova_TC_2010.png\", bbox_inches='tight', dpi=350)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c85bcd8b87f07ff9ffc2080eefd5e2fac2e62948c9af1b92f4d2a3164f63b006"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
