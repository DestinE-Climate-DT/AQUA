{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tempest Extreme experiment with NextGEMS data\n",
    "Data have been already preprocessed to 1x1 grid on Levante\n",
    "\n",
    "We start by reading and putting together into a single netcdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "import subprocess\n",
    "from time import time\n",
    "\n",
    "\n",
    "def readwrite_from_lowres(filein, fileout) : \n",
    "\n",
    "    \"\"\"\n",
    "    Read and write low resolution data to mimic access from FDB\n",
    "\n",
    "    Args: \n",
    "        filein: input file at low resolution\n",
    "        fileout: input file at low resolution (netcdf)\n",
    "\n",
    "    Returns: \n",
    "        outdict: dictionary with variable and dimensiona names for fileout\n",
    "    \"\"\"\n",
    "\n",
    "    xfield = xr.open_mfdataset(filein)\n",
    "\n",
    "    # check if output file exists\n",
    "    if os.path.exists(fileout):\n",
    "        os.remove(fileout)\n",
    "\n",
    "    xfield.to_netcdf(fileout)\n",
    "    xfield.close()\n",
    "    \n",
    "    outdict = {'lon': 'lon', 'lat': 'lat', \n",
    "            'psl': 'MSL', 'zg': 'Z',\n",
    "            'uas': 'U10M', 'vas': 'V10M'}\n",
    "\n",
    "    return outdict\n",
    "\n",
    "\n",
    "def run_detect_nodes(tempest_dictionary, tempest_filein, tempest_fileout) : \n",
    "\n",
    "    \"\"\"\"\n",
    "    Basic function to call from command line tempest extremes DetectNodes\n",
    "    \"\"\"\n",
    "    \n",
    "    detect_string= f'DetectNodes --in_data {tempest_filein} --timefilter 6hr --out {tempest_fileout} --searchbymin {tempest_dictionary[\"psl\"]} ' \\\n",
    "    f'--closedcontourcmd {tempest_dictionary[\"psl\"]},200.0,5.5,0;_DIFF({tempest_dictionary[\"zg\"]}(30000Pa),{tempest_dictionary[\"zg\"]}(50000Pa)),-58.8,6.5,1.0 --mergedist 6.0 ' \\\n",
    "    f'--outputcmd {tempest_dictionary[\"psl\"]},min,0;_VECMAG({tempest_dictionary[\"uas\"]},{tempest_dictionary[\"vas\"]}),max,2 --latname {tempest_dictionary[\"lat\"]} --lonname {tempest_dictionary[\"lon\"]}'\n",
    "\n",
    "    subprocess.run(detect_string.split(), stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "\n",
    "    return detect_string\n",
    "\n",
    "def read_lonlat_nodes(tempest_fileout) :\n",
    "\n",
    "    \"\"\"\n",
    "    Read from txt files output of DetectNodes the position of the centers of the TCs\n",
    "    \"\"\"\n",
    "\n",
    "    # open the output file and extract the required lon/lat\n",
    "    with open(tempest_fileout) as f:\n",
    "       data = f.readlines()\n",
    "    slon =[]\n",
    "    slat = []\n",
    "    for line in data[1:] : \n",
    "        slon = slon + [line.split('\\t')[3]]\n",
    "        slat = slat + [line.split('\\t')[4]]\n",
    "    out = {'lon' : slon, 'lat' : slat}\n",
    "    return out\n",
    "\n",
    "def lonlatbox(lon, lat, delta) : \n",
    "    \"\"\"\n",
    "    Define the list for the box\n",
    "    \"\"\"\n",
    "    return [float(lon) - delta, float(lon) +delta, float(lat) -delta, float(lat) + delta]\n",
    "\n",
    "def store_fullres_field(mfield, xfield, nodes, boxdim = 10): \n",
    "\n",
    "    \"\"\"Create xarray element that keep only the values of a field around the TC nodes\"\"\"\n",
    "\n",
    "    mask = xfield * 0\n",
    "    for k in range(0, len(nodes['lon'])) :\n",
    "        # add safe condition: keep only data between 50S and 50N\n",
    "        if (float(nodes['lat'][k]) > -50) and (float(nodes['lat'][k]) < 50): \n",
    "            box = lonlatbox(nodes['lon'][k], nodes['lat'][k], boxdim)\n",
    "            mask = mask + xr.where((xfield.lon > box[0]) & (xfield.lon < box[1]) & (xfield.lat > box[2]) & (xfield.lat < box[3]), True, False)\n",
    "\n",
    "    outfield = xfield.where(mask>0)\n",
    "\n",
    "    if isinstance(mfield, xr.DataArray):\n",
    "        outfield = xr.concat([mfield, outfield], dim = 'time')\n",
    "    \n",
    "    return outfield\n",
    "\n",
    "def write_fullres_field(gfield, filestore): \n",
    "\n",
    "    #compression = {'MSL': {'zlib': True}}\n",
    "    compression = {'MSL': {'zlib': True}}\n",
    "    gfield.where(gfield!=0).to_netcdf(filestore, encoding=compression)\n",
    "    gfield.close()\n",
    "\n",
    "def clean_files(filelist):\n",
    "\n",
    "    for fileout in filelist :\n",
    "        if os.path.exists(fileout):\n",
    "            os.remove(fileout)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pa# input directory\n",
    "indir='/home/b/b382216/scratch/regrid'\n",
    "tmpdir='/home/b/b382216/scratch/tmpdir'\n",
    "fulldir='/home/b/b382216/scratch/fullres'\n",
    "\n",
    "time = '000120'\n",
    "infile = os.path.join(indir, f'regrid+{time}_*.nc')\n",
    "outfile = os.path.join(tmpdir, time + '.nc')\n",
    "txtfile = os.path.join(tmpdir, 'output_' + time + '.txt')\n",
    "mslfile=os.path.join(fulldir, f'ICMGGhqys+{time}_msl.nc')\n",
    "storefile = os.path.join(tmpdir, 'TC_' + time + '.nc')\n",
    "\n",
    "# read and save the netcdf file to mimic the time lost by the FDB query\n",
    "tempest_dictionary = readwrite_from_lowres(infile, outfile)\n",
    "tempest_command = run_detect_nodes(tempest_dictionary, outfile, txtfile)\n",
    "tempest_nodes = read_lonlat_nodes(txtfile) \n",
    "\n",
    "xfield = xr.open_mfdataset(outfile)['MSL']\n",
    "\n",
    "gfield = xfield * 0\n",
    "for k in range(0, len(tempest_nodes['lon'])) :\n",
    "    box = lonlatbox(tempest_nodes['lon'][k], tempest_nodes['lat'][k], 10)\n",
    "    gfield = gfield + xr.where((xfield.lon > box[0]) & (xfield.lon < box[1]) & (xfield.lat > box[2]) & (xfield.lat < box[3]), True, False)\n",
    "\n",
    "xfield = xfield.where(gfield>0)\n",
    "\n",
    "xfield.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000\n",
      "Timestep done in 0.2662 seconds\n",
      "000120\n",
      "Timestep done in 0.0937 seconds\n",
      "000240\n",
      "Timestep done in 0.1170 seconds\n",
      "000360\n",
      "Timestep done in 0.2126 seconds\n",
      "001\n",
      "Storing output\n",
      "Storing daily data done in 8.3319 seconds\n",
      "000480\n",
      "Timestep done in 0.1165 seconds\n",
      "000600\n",
      "Timestep done in 0.0911 seconds\n",
      "000720\n",
      "Timestep done in 0.1035 seconds\n",
      "000840\n",
      "Timestep done in 0.0853 seconds\n",
      "002\n",
      "Storing output\n",
      "Storing daily data done in 6.6761 seconds\n",
      "000960\n",
      "Timestep done in 0.6809 seconds\n",
      "001080\n",
      "Timestep done in 0.1007 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# input directory\n",
    "indir='/home/b/b382216/scratch/regrid'\n",
    "tmpdir='/home/b/b382216/scratch/tmpdir'\n",
    "fulldir='/home/b/b382216/scratch/fullres'\n",
    "#dask.config.set(scheduler=\"synchronous\")\n",
    "\n",
    "# init the file to be stored\n",
    "xfield = 0\n",
    "# loop on timerecors\n",
    "for t in range(0, 12000, 120) : \n",
    "    tic = time()\n",
    "\n",
    "    tttt = str(t).zfill(6)\n",
    "    print(tttt)\n",
    "    # path definition\n",
    "    original_file = os.path.join(indir, f'regrid+{tttt}_*.nc')\n",
    "    netcdf_file = os.path.join(tmpdir, tttt + '.nc')\n",
    "    txt_file = os.path.join(tmpdir, 'output_' + tttt + '.txt')\n",
    "    msl_fullres_file=os.path.join(fulldir, f'ICMGGhqys+{tttt}_msl.nc')\n",
    "   \n",
    "\n",
    "    # read and save the netcdf file to mimic the time lost by the FDB query\n",
    "    tempest_dictionary = readwrite_from_lowres(original_file, netcdf_file)\n",
    "\n",
    "    # run tempest extremes\n",
    "    tempest_command = run_detect_nodes(tempest_dictionary, netcdf_file, txt_file)\n",
    "    tempest_nodes = read_lonlat_nodes(txt_file)\n",
    "\n",
    "    # get the full res field and store the required values around the Nodes\n",
    "    fullres_field = xr.open_mfdataset(msl_fullres_file)['MSL']\n",
    "    xfield = store_fullres_field(xfield, fullres_field, tempest_nodes)\n",
    "    clean_files([netcdf_file])\n",
    "    \n",
    "    toc = time()\n",
    "    print('Timestep done in {:.4f} seconds'.format(toc - tic))\n",
    "\n",
    "    # store data every day\n",
    "    if (t+120) % (120*4) == 0 : \n",
    "        day = str((t+120) // (120*4)).zfill(3)\n",
    "        print(day)\n",
    "        print('Storing output')\n",
    "        tic = time()\n",
    "\n",
    "        # store the file\n",
    "        store_file = os.path.join(tmpdir, f'TC_MSL_{day}.nc')\n",
    "        write_fullres_field(xfield, store_file)\n",
    "\n",
    "        #reinit xfield\n",
    "        xfield = 0\n",
    "\n",
    "        toc = time()\n",
    "        print('Storing daily data done in {:.4f} seconds'.format(toc - tic))\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the lon/lat from the txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this diagnostic to test the new aqua.docker.rundiag() function\n",
    "# Everything is defined in the diagnostic.yaml and machine.yaml files\n",
    "# If no argument is provided then the first command defined in machine.yaml is run\n",
    "#output = aqua.docker.rundiag(cmd=\"detect\")\n",
    "\n",
    "# replace the dokker structure with a python environment which does the following:\n",
    "\n",
    "# detect_out = aqua.docker.rundiag(cmd=\"detect\")\n",
    "# stitch_out = aqua.docker.rundiag(cmd=\"stitch\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from aqua import regrid\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import re\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from cartopy import config\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "#import tempest_lib as tmp\n",
    "\n",
    "\n",
    "diagname = 'TCs'\n",
    "machine = 'levante'\n",
    "\n",
    "\n",
    "with open(f'../../AQUA/diagnostics/config/config_{machine}.yml', 'r', encoding='utf-8') as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "with open(f'{diagname}.yml', 'r', encoding='utf-8') as file:\n",
    "    namelist = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "#regrid files from original res to 1x1\n",
    "\n",
    "from cdo import *   # python version\n",
    "cdo = Cdo()\n",
    "\n",
    "k_init=000000\n",
    "k_final=166560\n",
    "#loop to regrid files \n",
    "for k in range(k_init,k_final):\n",
    "\n",
    "        print(\"GH regridding\")\n",
    "        cdo genbil,r360x180 file_in weights.nc\n",
    "        cdo remap, r360x180, weights file_in file_out\n",
    "\n",
    "        print(\"lnsp regridding\")\n",
    "        print(\"10u regridding\")\n",
    "        print(\"10v regridding\")\n",
    "\n",
    "\n",
    "# ADJUST TEMPEST COMMANDS IN THE RESPECTIVE YML FILES\n",
    "\n",
    "# first run detect_nodes to detect the TCs centers\n",
    "subprocess.run(['bash', './detect_nodes'])\n",
    "\n",
    "# then compute trajectories using stitch_nodes\n",
    "subprocess.run(['bash', './stitch_nodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/zarzycki/cymep\n",
    "\n",
    "def getTrajectories(filename,nVars,headerDelimStr,isUnstruc):\n",
    "  print(\"Getting trajectories from TempestExtremes file...\")\n",
    "  print(\"Running getTrajectories on '%s' with unstruc set to '%s'\" % (filename, isUnstruc))\n",
    "  print(\"nVars set to %d and headerDelimStr set to '%s'\" % (nVars, headerDelimStr))\n",
    "\n",
    "  # Using the newer with construct to close the file automatically.\n",
    "  with open(filename) as f:\n",
    "      data = f.readlines()\n",
    "\n",
    "  # Find total number of trajectories and maximum length of trajectories\n",
    "  numtraj=0\n",
    "  numPts=[]\n",
    "  for line in data:\n",
    "    if headerDelimStr in line:\n",
    "      # if header line, store number of points in given traj in numPts\n",
    "      headArr = line.split()\n",
    "      numtraj += 1\n",
    "      numPts.append(int(headArr[1]))\n",
    "    else:\n",
    "      # if not a header line, and nVars = -1, find number of columns in data point\n",
    "      if nVars < 0:\n",
    "        nVars=len(line.split())\n",
    "  \n",
    "  maxNumPts = max(numPts) # Maximum length of ANY trajectory\n",
    "\n",
    "  print(\"Found %d columns\" % nVars)\n",
    "  print(\"Found %d trajectories\" % numtraj)\n",
    "\n",
    "  # Initialize storm and line counter\n",
    "  stormID=-1\n",
    "  lineOfTraj=-1\n",
    "\n",
    "  # Create array for data\n",
    "  if isUnstruc:\n",
    "    prodata = np.empty((nVars+1,numtraj,maxNumPts))\n",
    "  else:\n",
    "    prodata = np.empty((nVars,numtraj,maxNumPts))\n",
    "\n",
    "  prodata[:] = np.NAN\n",
    "\n",
    "  for i, line in enumerate(data):\n",
    "    if headerDelimStr in line:  # check if header string is satisfied\n",
    "      stormID += 1      # increment storm\n",
    "      lineOfTraj = 0    # reset trajectory line to zero\n",
    "    else:\n",
    "      ptArr = line.split()\n",
    "      for jj in range(nVars):\n",
    "        if isUnstruc:\n",
    "          prodata[jj+1,stormID,lineOfTraj]=ptArr[jj]\n",
    "        else:\n",
    "          prodata[jj,stormID,lineOfTraj]=ptArr[jj]\n",
    "      lineOfTraj += 1   # increment line\n",
    "\n",
    "  print(\"... done reading data\")\n",
    "  return numtraj, maxNumPts, prodata\n",
    "\n",
    "\n",
    "def getNodes(filename,nVars,isUnstruc):\n",
    "  print(\"Getting nodes from TempestExtremes file...\")\n",
    "\n",
    "  # Using the newer with construct to close the file automatically.\n",
    "  with open(filename) as f:\n",
    "      data = f.readlines()\n",
    "\n",
    "  # Find total number of trajectories and maximum length of trajectories\n",
    "  numnodetimes=0\n",
    "  numPts=[]\n",
    "  for line in data:\n",
    "    if re.match(r'\\w', line):\n",
    "      # if header line, store number of points in given traj in numPts\n",
    "      headArr = line.split()\n",
    "      numnodetimes += 1\n",
    "      numPts.append(int(headArr[3]))\n",
    "    else:\n",
    "      # if not a header line, and nVars = -1, find number of columns in data point\n",
    "      if nVars < 0:\n",
    "        nVars=len(line.split())\n",
    "  \n",
    "  maxNumPts = max(numPts) # Maximum length of ANY trajectory\n",
    "\n",
    "  print(\"Found %d columns\" % nVars)\n",
    "  print(\"Found %d trajectories\" % numnodetimes)\n",
    "  print(\"Found %d maxNumPts\" % maxNumPts)\n",
    "\n",
    "  # Initialize storm and line counter\n",
    "  stormID=-1\n",
    "  lineOfTraj=-1\n",
    "\n",
    "  # Create array for data\n",
    "  if isUnstruc:\n",
    "    prodata = np.empty((nVars+5,numnodetimes,maxNumPts))\n",
    "  else:\n",
    "    prodata = np.empty((nVars+4,numnodetimes,maxNumPts))\n",
    "\n",
    "  prodata[:] = np.NAN\n",
    "\n",
    "  nextHeadLine=0\n",
    "  for i, line in enumerate(data):\n",
    "    if re.match(r'\\w', line):  # check if header string is satisfied\n",
    "      stormID += 1      # increment storm\n",
    "      lineOfTraj = 0    # reset trajectory line to zero\n",
    "      headArr = line.split()\n",
    "      YYYY = int(headArr[0])\n",
    "      MM = int(headArr[1])\n",
    "      DD = int(headArr[2])\n",
    "      HH = int(headArr[4])\n",
    "    else:\n",
    "      ptArr = line.split()\n",
    "      for jj in range(nVars-1):\n",
    "        if isUnstruc:\n",
    "          prodata[jj+1,stormID,lineOfTraj]=ptArr[jj]\n",
    "        else:\n",
    "          prodata[jj,stormID,lineOfTraj]=ptArr[jj]\n",
    "      if isUnstruc:\n",
    "        prodata[nVars+1,stormID,lineOfTraj]=YYYY\n",
    "        prodata[nVars+2,stormID,lineOfTraj]=MM\n",
    "        prodata[nVars+3,stormID,lineOfTraj]=DD\n",
    "        prodata[nVars+4,stormID,lineOfTraj]=HH\n",
    "      else:\n",
    "        prodata[nVars  ,stormID,lineOfTraj]=YYYY\n",
    "        prodata[nVars+1,stormID,lineOfTraj]=MM\n",
    "        prodata[nVars+2,stormID,lineOfTraj]=DD\n",
    "        prodata[nVars+3,stormID,lineOfTraj]=HH\n",
    "      lineOfTraj += 1   # increment line\n",
    "\n",
    "  print(\"... done reading data\")\n",
    "  return numnodetimes, maxNumPts, prodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempest settings\n",
    "nVars=-1\n",
    "headerStr='start'\n",
    "isUnstruc = 0\n",
    "\n",
    "# Extract trajectories from tempest file and assign to arrays\n",
    "# USER_MODIFY\n",
    "nstorms, ntimes, traj_data = getTrajectories(trajfile,nVars,headerStr,isUnstruc)\n",
    "xlon   = traj_data[2,:,:]\n",
    "xlat   = traj_data[3,:,:]\n",
    "xpres  = traj_data[4,:,:]/100.\n",
    "xwind  = traj_data[5,:,:]\n",
    "xyear  = traj_data[7,:,:]\n",
    "xmonth = traj_data[8,:,:]\n",
    "\n",
    "# Initialize axes\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "# ax.set_extent([-180, 180, -75, 75], crs=None)\n",
    "\n",
    "# Set title and subtitle\n",
    "plt.title('Example of a Trajectory Plot')\n",
    "\n",
    "\n",
    "# Set land feature and change color to 'lightgrey'\n",
    "# See link for extensive list of colors:\n",
    "# https://matplotlib.org/3.1.0/gallery/color/named_colors.html\n",
    "ax.add_feature(cfeature.LAND, color='lightgrey')\n",
    "\n",
    "gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                  linewidth=0.5, color='k', alpha=0.5, linestyle='--')\n",
    "gl.xlabels_top = False\n",
    "gl.ylabels_left = False\n",
    "#gl.xlines = False\n",
    "gl.xlocator = mticker.FixedLocator([-180, -90, 0, 90, 180])\n",
    "gl.xformatter = LONGITUDE_FORMATTER\n",
    "gl.yformatter = LATITUDE_FORMATTER\n",
    "gl.ylabel_style = {'size': 12, 'color': 'black'}\n",
    "gl.xlabel_style = {'size': 12, 'color': 'black'}\n",
    "\n",
    "\n",
    "\n",
    "# Plot each trajectory\n",
    "for i in range(nstorms):\n",
    "\n",
    "        # doesn't work with cartopy!\n",
    "        #plt.plot(xlon[i], xlat[i], linewidth=0.4)\n",
    "\n",
    "\n",
    "\n",
    "    plt.scatter(x=xlon[i], y=xlat[i],\n",
    "                                                color=\"black\",\n",
    "                                                s=30,\n",
    "                                                linewidths=0.5,\n",
    "                                                marker=\".\",\n",
    "                                                alpha=0.8,\n",
    "                                                transform=ccrs.PlateCarree()) ## Important\n",
    "\n",
    "\n",
    "plt.savefig(plotdir + \"prova_TC_2010.png\", bbox_inches='tight', dpi=350)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c85bcd8b87f07ff9ffc2080eefd5e2fac2e62948c9af1b92f4d2a3164f63b006"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
