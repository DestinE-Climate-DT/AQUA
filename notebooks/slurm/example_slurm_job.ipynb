{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of slurm job creation and submission (on levante)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "0. [Install the following:]()\n",
    "\n",
    "    - [dask]() (conda)\n",
    "\n",
    "    - [dask_jobqueue]() (pip)\n",
    "\n",
    "    - [os, re]()\n",
    "\n",
    "1. [Importing the slurm_job module](#1-import-the-slurm_job-module)\n",
    "\n",
    "2. [Creating the Slurm Job on LEVANTE](#2-creating-the-slurm-job-on-levante)\n",
    "\n",
    "    2.1 [Creating the Job with manual selection of cores, memory, and walltime](#21-creating-the-job-with-manual-selection-of-cores-memory-and-walltime)\n",
    "\n",
    "    2.2 [Creating the Job with exclusive node access](#22-creating-the-job-with-exclusive-node-access)\n",
    "\n",
    "    2.3 [Creating the Job with maxumum availibale resources per the node](#23-creating-the-job-with-maxumum-availibale-resources-per-the-node)\n",
    "\n",
    "    2.4 [Redirecting the SLURM output to /any/path/you/want](#24-redirecting-the-slurm-output-to-anypathyouwant)\n",
    "\n",
    "3. [Creating and Submitting the Job to the SLURM queue on Lumi](#3-creating-and-submitting-the-job-to-the-slurm-queue-on-lumi)\n",
    "    \n",
    "4. [Canceling the Slurm Job](#4-canceling-the-slurm-job)\n",
    "\n",
    "    3.1 [Canceling all jobs of the user](#41-cancelling-all-jobs-of-user)\n",
    "\n",
    "    3.2 [Canceling the specific Job](#42-canceling-specific-job)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the slurm_job module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The `slurm_job` module contains the following function `squeue`, `slurm_job`, `max_resources_per_node`, `scancel`,  which allows us to create and operate the Slurm Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slurm import  max_resources_per_node, squeue, slurm_job, scancel, output_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the Slurm Job on LEVANTE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating the Job with manual selection of cores, memory, and walltime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p compute\n",
      "#SBATCH -A bb1153\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=1\n",
      "#SBATCH --mem=10G\n",
      "#SBATCH -t 02:30:00\n",
      "#SBATCH --error=./slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=./slurm/output/dask-worker-%j.out\n",
      "\n",
      "/home/b/b382267/mambaforge/envs/tropical-rainfall/bin/python -m distributed.cli.dask_worker tcp://136.172.124.6:35899 --nthreads 1 --memory-limit 9.31GiB --name dummy-name --nanny --death-timeout 60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "slurm_job()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can check the status of created Job in the queue using the function `squeue()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           4945327   compute dask-wor  b382267 PD       0:00      1 (Priority)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squeue()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By default, the Job has the following attributes:\n",
    " - cores    = 1, \n",
    " - memory   = \"10 GB\", \n",
    " - queue    = \"compute\", \n",
    " - walltime = '02:30:00', \n",
    " - jobs     = 1\n",
    "\n",
    " \n",
    "##### If you want to use a different amount of cores, memory, wall time, jobs, or a different queue, you can specify it as an argument of function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_job(cores=8, memory=\"50 GB\", queue = \"interactive\", walltime='00:30:00', jobs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Creating the Job with exclusive node access"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The function has an argument `exclusive`, which is False by default.  If we set the argument to True, we will get exclusive access to the node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_job(exclusive=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The exclusive argument DOES NOT automatically provide us the maximum available memory, number of cores, and walltime! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_job(exclusive=True, cores=256, memory=\"500 GB\", queue = \"interactive\", walltime='12:00:00', jobs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Creating the Job with maxumum availibale resources per the node"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The function has an argument `max_resources_per_node`, which is False by default. If we set the argument to True, the number of cores, memory, and walltime will equal the maximum number of cores, memory, and walltime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p compute\n",
      "#SBATCH -A bb1153\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=256\n",
      "#SBATCH --mem=235G\n",
      "#SBATCH -t 8:00:00\n",
      "#SBATCH --error=./slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=./slurm/output/dask-worker-%j.out\n",
      "\n",
      "/home/b/b382267/mambaforge/envs/tropical-rainfall/bin/python -m distributed.cli.dask_worker tcp://136.172.124.6:45169 --nthreads 16 --nworkers 16 --memory-limit 14.63GiB --name dummy-name --nanny --death-timeout 60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "slurm_job(max_resources=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With the argument `max_resources_per_node=True`, the function `max_resources_per_node()` automatically extracts the following information about the resources in any queue:\n",
    "\n",
    " - Size of memory per node in Gigabytes: `max_memory`, \n",
    "\n",
    " - Maximum time for any job in the format \"days-hours:minutes:seconds: `max_walltime`\n",
    "\n",
    " - Number of CPUs per node: `max_cpus`, \n",
    "\n",
    " - Number of sockets per node: `max_sockets`\n",
    "\n",
    " - Number of cores per socket: `max_cores`, \n",
    " \n",
    " - Number of threads per core: `max_threads`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('251.3671875 GB', '8:00:00', '256', '8', '16', '2')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_resources_per_node('compute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('502.9296875 GB', '12:00:00', '256', '8', '16', '2')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_resources_per_node('interactive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Redirecting the SLURM output to /any/path/you/want"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Slurm Job writing by default\n",
    "    - the errors into `./slurm/logs` directory \n",
    "    - the output into `./slurm/output/` directory\n",
    "\n",
    "##### If folders `/slurm`,  `/slurm/output`,  `/slurm/logs` do not exist, the function will create them automatically. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### But user can specify the path to redirect the SLURM output from the currect folder to `/any/other/folder/` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_job(path_to_output='/any/path/you/want')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating and Submitting the Job to the SLURM queue on Lumi (Under development)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### You need to change the project name if you want to create the Slurm Job on Lumi with the function `slurm_job.` Currently, it  is `account=\"bb1153\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_job(account=='Your_Lumi_account_name')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Canceling the Slurm Job"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Cancelling all jobs of user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scancel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Canceling specific job"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Knowing the Job_ID, you can cancel your Job in the queue. For exaple, you can find your Job_ID using the function `squeue().` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_ID = 4929434\n",
    "scancel(Job_ID)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the status of canceled Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           4945492   compute dask-wor  b382267  R       0:08      1 l40358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squeue()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tropical-rainfall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
