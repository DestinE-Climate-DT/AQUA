{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of slurm job creation and submission"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "1. [Importing the job module](#1-importing-the-job-module)\n",
    "\n",
    "2. [Creating the Slurm Job](#2-creating-the-slurm-job)\n",
    "\n",
    "    2.1 [Creating the Job with manual selection of cores, memory, and walltime](#21-creating-the-job-with-manual-selection-of-cores-memory-and-walltime)\n",
    "\n",
    "    2.2 [Creating the Job with exclusive node access](#22-creating-the-job-with-exclusive-node-access)\n",
    "\n",
    "    2.3 [Creating the Job with maxumum availibale resources per the node](#23-creating-the-job-with-maxumum-availibale-resources-per-the-node)\n",
    "\n",
    "    2.4 [Redirecting the SLURM output to /any/path/you/want](#24-redirecting-the-slurm-output-to-anypathyouwant)\n",
    "\n",
    "3. [Creating and Submitting the Job to the SLURM queue on Lumi](#3-creating-and-submitting-the-job-to-the-slurm-queue-on-lumi)\n",
    "    \n",
    "4. [Canceling the Slurm Job](#4-canceling-the-slurm-job)\n",
    "\n",
    "    3.1 [Canceling all jobs of the user](#41-cancelling-all-jobs-of-user)\n",
    "\n",
    "    3.2 [Canceling the specific Job](#42-canceling-specific-job)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the job module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `job` module contains the following functions: `squeue`, `job`, `max_resources_per_node`, `scancel`, which allows us to create and operate the Slurm Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aqua.slurm import slurm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the Slurm Job"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating the Job with manual selection of cores, memory, and walltime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `slurm.job()` allows to create a job in the slurm queue. \n",
    "It works both on Lumi and Levante.\n",
    "Some options are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/LUMI_TYKKY_eiW6rEt/miniconda/envs/env1/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 41381 instead\n",
      "  warnings.warn(\n",
      "\u001b[38;2;255;165;0m2024-05-08 18:34:40 :: slurm :: WARNING  -> #!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p small\n",
      "#SBATCH -A project_465000454\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=1\n",
      "#SBATCH --mem=10G\n",
      "#SBATCH -t 02:30:00\n",
      "#SBATCH --error=./slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=./slurm/output/dask-worker-%j.out\n",
      "\n",
      "/users/nazarova/mambaforge/aqua/bin/python3.11 -m distributed.cli.dask_worker tcp://193.167.209.165:41519 --name dummy-name --nthreads 1 --memory-limit 9.31GiB --nanny --death-timeout 60\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "slurm.job()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the status of created Job in the queue using the function `squeue()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID      CPUS  NODES ST         NAME                 TIME       START_TIME           DEPENDENCY           PARTITION            MIN_MEMORY          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.squeue()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cancel all the jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.scancel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the Job has the following attributes:\n",
    "\n",
    "- `exclusive=False`: If True, the job will be submitted asking for exclusive access to the node.\n",
    "- `max_resources=False`: If True, the job will be submitted asking for the maximum resources available on the node.\n",
    "- `cores=1`: number of cores per socket.\n",
    "- `memory=\"10 GB\"`: real memory required per node.\n",
    "- `walltime=\"02:30:00\"`: duration of the allocation.\n",
    "- `jobs=1`: factor of assignment scaling across multiple nodes,\n",
    "- `path_to_output=\".\"`: path where log, err and output files are stored,\n",
    "- `account=\"project_465000454\"`: the submission queue for job and resource allocation,\n",
    "- `queue=\"small\"`: project account under which the job runs.\n",
    " \n",
    "If you want to use a different amount of cores, memory, wall time, jobs, or a different queue, you can specify it as an argument of function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/LUMI_TYKKY_eiW6rEt/miniconda/envs/env1/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 39135 instead\n",
      "  warnings.warn(\n",
      "\u001b[38;2;255;165;0m2024-05-08 18:34:41 :: slurm :: WARNING  -> #!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p interactive\n",
      "#SBATCH -A project_465000454\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=8\n",
      "#SBATCH --mem=47G\n",
      "#SBATCH -t 00:30:00\n",
      "#SBATCH --error=./slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=./slurm/output/dask-worker-%j.out\n",
      "\n",
      "/users/nazarova/mambaforge/aqua/bin/python3.11 -m distributed.cli.dask_worker tcp://193.167.209.165:38017 --name dummy-name --nthreads 2 --memory-limit 11.64GiB --nworkers 4 --nanny --death-timeout 60\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "slurm.job(cores=8, memory=\"50 GB\", queue=\"interactive\", walltime='00:30:00', jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID      CPUS  NODES ST         NAME                 TIME       START_TIME           DEPENDENCY           PARTITION            MIN_MEMORY          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.squeue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;64;184;50m2024-05-08 18:34:41 :: slurm :: INFO     -> Cancelling all user jobs in the queue\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "slurm.scancel(loglevel=\"INFO\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Creating the Job with exclusive node access"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function has an argument `exclusive`, which is False by default.  If we set the argument to True, we will get exclusive access to the node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/LUMI_TYKKY_eiW6rEt/miniconda/envs/env1/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 36525 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;255;165;0m2024-05-08 18:34:41 :: slurm :: WARNING  -> #!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p small\n",
      "#SBATCH -A project_465000454\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=1\n",
      "#SBATCH --mem=10G\n",
      "#SBATCH -t 02:30:00\n",
      "#SBATCH --error=./slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=./slurm/output/dask-worker-%j.out\n",
      "#SBATCH --get-user-env\n",
      "#SBATCH --exclusive\n",
      "\n",
      "/users/nazarova/mambaforge/aqua/bin/python3.11 -m distributed.cli.dask_worker tcp://193.167.209.165:35275 --name dummy-name --nthreads 1 --memory-limit 9.31GiB --nanny --death-timeout 60\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "slurm.job(exclusive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID      CPUS  NODES ST         NAME                 TIME       START_TIME           DEPENDENCY           PARTITION            MIN_MEMORY          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.squeue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;64;184;50m2024-05-08 18:34:41 :: slurm :: INFO     -> Cancelling the job with ID: 7054951\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "slurm.scancel(Job_ID=7054951, loglevel=\"INFO\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `exclusive` argument DOES NOT automatically provide us the maximum available memory, number of cores, and walltime!\n",
    "It only provide you the exclusive usage of the node, meaning that no other job can run at the same time on the same node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/LUMI_TYKKY_eiW6rEt/miniconda/envs/env1/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 37191 instead\n",
      "  warnings.warn(\n",
      "\u001b[38;2;255;165;0m2024-05-08 18:34:41 :: slurm :: WARNING  -> #!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p interactive\n",
      "#SBATCH -A project_465000454\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=32\n",
      "#SBATCH --mem=187G\n",
      "#SBATCH -t 8:00:00\n",
      "#SBATCH --error=./slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=./slurm/output/dask-worker-%j.out\n",
      "#SBATCH --get-user-env\n",
      "#SBATCH --exclusive\n",
      "\n",
      "/users/nazarova/mambaforge/aqua/bin/python3.11 -m distributed.cli.dask_worker tcp://193.167.209.165:45407 --name dummy-name --nthreads 4 --memory-limit 23.28GiB --nworkers 8 --nanny --death-timeout 60\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "slurm.job(exclusive=True, cores=32, memory=\"200 GB\", queue = \"interactive\", walltime='8:00:00', jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID      CPUS  NODES ST         NAME                 TIME       START_TIME           DEPENDENCY           PARTITION            MIN_MEMORY          \n",
      "7056337    32    1     PD         dask-worker          0:00       N/A                  (null)               interactive          187G                \n",
      "7056336    1     1     PD         dask-worker          0:00       N/A                  (null)               small                10G                 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.squeue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.scancel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Creating the Job with maxumum availibale resources per the node"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function has an argument `max_resources_per_node`, which is `False` by default. If we set the argument to `True`, the number of cores, memory, and walltime will equal the maximum allowed by the selected queue/partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('99.19921875 GB', '3-00:00:00', '128', '2', '64', '2')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.max_resources_per_node(queue='small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/LUMI_TYKKY_eiW6rEt/miniconda/envs/env1/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 43235 instead\n",
      "  warnings.warn(\n",
      "\u001b[38;2;255;165;0m2024-05-09 17:06:55 :: slurm :: WARNING  -> #!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p small\n",
      "#SBATCH -A project_465000454\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=128\n",
      "#SBATCH --mem=93G\n",
      "#SBATCH -t 3-00:00:00\n",
      "#SBATCH --error=./slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=./slurm/output/dask-worker-%j.out\n",
      "\n",
      "/users/nazarova/mambaforge/aqua/bin/python3.11 -m distributed.cli.dask_worker tcp://193.167.209.164:46565 --name dummy-name --nthreads 8 --memory-limit 5.77GiB --nworkers 16 --nanny --death-timeout 60\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "slurm.job(max_resources=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID      CPUS  NODES ST         NAME                 TIME       START_TIME           DEPENDENCY           PARTITION            MIN_MEMORY          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.squeue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.scancel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the argument `max_resources_per_node=True`, the function `max_resources_per_node()` automatically extracts the following information about the resources in any queue:\n",
    "\n",
    " - Size of memory per node in Gigabytes: `max_memory`, \n",
    "\n",
    " - Maximum time for any job in the format \"days-hours:minutes:seconds: `max_walltime`\n",
    "\n",
    " - Number of CPUs per node: `max_cpus`, \n",
    "\n",
    " - Number of sockets per node: `max_sockets`\n",
    "\n",
    " - Number of cores per socket: `max_cores`, \n",
    " \n",
    " - Number of threads per core: `max_threads`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function can be also used alone to have info about a specific queue/partition.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('99.19921875 GB', '3-00:00:00', '128', '2', '64', '2')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.max_resources_per_node('small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('224.0 GB', '8:00:00', '128', '2', '64', '2')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.max_resources_per_node('interactive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Redirecting the SLURM output to `/any/path/you/want`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slurm Job writes by default\n",
    "    - the errors into `./slurm/logs` directory \n",
    "    - the output into `./slurm/output/` directory\n",
    "\n",
    "If folders `/slurm`,  `/slurm/output`,  `/slurm/logs` do not exist, the function will create them automatically. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user can specify a custom path to redirect the SLURM outputs with the `path_to_output` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.job(path_to_output='/any/path/you/want')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating and Submitting the Job to the SLURM queue on Lumi (Under development)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to change the account name if you want to create the Slurm Job on Lumi with the function `job()`. Currently on lumi, it  is `account=\"project_465000454\"` and it is valid for the whole project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.job(account='Your_Lumi_account_name')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Canceling the Slurm Job"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Cancelling all jobs of user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.scancel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Canceling specific job"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Knowing the Job_ID, you can cancel your Job in the queue. For exaple, you can find your Job_ID using the function `squeue().` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_ID = 4929434\n",
    "slurm.scancel(Job_ID)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the status of Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID      CPUS  NODES ST         NAME                 TIME       START_TIME           DEPENDENCY           PARTITION            MIN_MEMORY          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.squeue()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tropical-rainfall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
