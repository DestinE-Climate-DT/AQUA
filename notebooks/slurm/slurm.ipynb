{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of slurm job creation and submission (on levante)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "1. [Importing the job module](#1-importing-the-job-module)\n",
    "\n",
    "2. [Creating the Slurm Job on LEVANTE](#2-creating-the-slurm-job-on-levante)\n",
    "\n",
    "    2.1 [Creating the Job with manual selection of cores, memory, and walltime](#21-creating-the-job-with-manual-selection-of-cores-memory-and-walltime)\n",
    "\n",
    "    2.2 [Creating the Job with exclusive node access](#22-creating-the-job-with-exclusive-node-access)\n",
    "\n",
    "    2.3 [Creating the Job with maxumum availibale resources per the node](#23-creating-the-job-with-maxumum-availibale-resources-per-the-node)\n",
    "\n",
    "    2.4 [Redirecting the SLURM output to /any/path/you/want](#24-redirecting-the-slurm-output-to-anypathyouwant)\n",
    "\n",
    "3. [Creating and Submitting the Job to the SLURM queue on Lumi](#3-creating-and-submitting-the-job-to-the-slurm-queue-on-lumi)\n",
    "    \n",
    "4. [Canceling the Slurm Job](#4-canceling-the-slurm-job)\n",
    "\n",
    "    3.1 [Canceling all jobs of the user](#41-cancelling-all-jobs-of-user)\n",
    "\n",
    "    3.2 [Canceling the specific Job](#42-canceling-specific-job)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the job module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `job` module contains the following functions: `squeue`, `job`, `max_resources_per_node`, `scancel`, which allows us to create and operate the Slurm Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aqua.slurm import slurm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the Slurm Job on LEVANTE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating the Job with manual selection of cores, memory, and walltime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `slurm.job()` allows to create a job in the slurm queue on levante. Some options are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 16:01:12 :: create_folder :: WARNING  -> Folder ./slurm already exists\n",
      "2023-05-18 16:01:12 :: create_folder :: WARNING  -> Folder ./slurm/logs already exists\n",
      "2023-05-18 16:01:12 :: create_folder :: WARNING  -> Folder ./slurm/output already exists\n",
      "/work/bb1153/b382289/mambaforge/envs/aqua/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 39323 instead\n",
      "  warnings.warn(\n",
      "2023-05-18 16:01:13 :: slurm :: WARNING  -> #!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p compute\n",
      "#SBATCH -A bb1153\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=1\n",
      "#SBATCH --mem=10G\n",
      "#SBATCH -t 02:30:00\n",
      "#SBATCH --error=./slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=./slurm/output/dask-worker-%j.out\n",
      "\n",
      "/work/bb1153/b382289/mambaforge/envs/aqua/bin/python -m distributed.cli.dask_worker tcp://136.172.124.5:45835 --nthreads 1 --memory-limit 9.31GiB --name dummy-name --nanny --death-timeout 60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "slurm.job()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the status of created Job in the queue using the function `squeue()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID      CPUS  NODES ST         NAME                 TIME       START_TIME           DEPENDENCY           PARTITION            MIN_MEMORY          \n",
      "5234111    1     1     PD         dask-worker          0:00       N/A                  (null)               compute              10G                 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.squeue()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cancel all the jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.scancel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the Job has the following attributes:\n",
    "\n",
    "- `exclusive=False`: If True, the job will be submitted asking for exclusive access to the node.\n",
    "- `max_resources=False`: If True, the job will be submitted asking for the maximum resources available on the node.\n",
    "- `cores=1`: number of cores per socket.\n",
    "- `memory=\"10 GB\"`: real memory required per node.\n",
    "- `queue=\"compute\"`: queue/partition to which SLURM submit the job.\n",
    "- `walltime=\"02:30:00\"`: duration of the allocation.\n",
    "- `jobs=1`: factor of assignment scaling across multiple nodes.\n",
    "- `account=bb1153`: account that submits the job. It is the project id on levante.\n",
    "- `path_to_output=\".\"`: path where log, err and output files are stored.\n",
    "\n",
    " \n",
    "If you want to use a different amount of cores, memory, wall time, jobs, or a different queue, you can specify it as an argument of function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 16:02:28 :: create_folder :: WARNING  -> Folder ./slurm already exists\n",
      "2023-05-18 16:02:28 :: create_folder :: WARNING  -> Folder ./slurm/logs already exists\n",
      "2023-05-18 16:02:28 :: create_folder :: WARNING  -> Folder ./slurm/output already exists\n",
      "/work/bb1153/b382289/mambaforge/envs/aqua/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 33993 instead\n",
      "  warnings.warn(\n",
      "2023-05-18 16:02:28 :: slurm :: WARNING  -> #!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p interactive\n",
      "#SBATCH -A bb1153\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=8\n",
      "#SBATCH --mem=47G\n",
      "#SBATCH -t 00:30:00\n",
      "#SBATCH --error=./slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=./slurm/output/dask-worker-%j.out\n",
      "\n",
      "/work/bb1153/b382289/mambaforge/envs/aqua/bin/python -m distributed.cli.dask_worker tcp://136.172.124.5:33661 --nthreads 2 --nworkers 4 --memory-limit 11.64GiB --name dummy-name --nanny --death-timeout 60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "slurm.job(cores=8, memory=\"50 GB\", queue=\"interactive\", walltime='00:30:00', jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID      CPUS  NODES ST         NAME                 TIME       START_TIME           DEPENDENCY           PARTITION            MIN_MEMORY          \n",
      "5234123    24    1     R          dask-worker          0:10       2023-05-18T16:02:29  (null)               interactive          47G                 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.squeue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 16:02:56 :: slurm :: INFO     -> Cancelling all user jobs in the queue\n"
     ]
    }
   ],
   "source": [
    "slurm.scancel(loglevel=\"INFO\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Creating the Job with exclusive node access"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function has an argument `exclusive`, which is False by default.  If we set the argument to True, we will get exclusive access to the node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 16:03:58 :: create_folder :: WARNING  -> Folder ./slurm already exists\n",
      "2023-05-18 16:03:58 :: create_folder :: WARNING  -> Folder ./slurm/logs already exists\n",
      "2023-05-18 16:03:58 :: create_folder :: WARNING  -> Folder ./slurm/output already exists\n",
      "/work/bb1153/b382289/mambaforge/envs/aqua/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 44411 instead\n",
      "  warnings.warn(\n",
      "2023-05-18 16:03:58 :: slurm :: WARNING  -> #!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p compute\n",
      "#SBATCH -A bb1153\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=1\n",
      "#SBATCH --mem=10G\n",
      "#SBATCH -t 02:30:00\n",
      "#SBATCH --error=./slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=./slurm/output/dask-worker-%j.out\n",
      "#SBATCH --get-user-env\n",
      "#SBATCH --exclusive\n",
      "\n",
      "/work/bb1153/b382289/mambaforge/envs/aqua/bin/python -m distributed.cli.dask_worker tcp://136.172.124.5:43577 --nthreads 1 --memory-limit 9.31GiB --name dummy-name --nanny --death-timeout 60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "slurm.job(exclusive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID      CPUS  NODES ST         NAME                 TIME       START_TIME           DEPENDENCY           PARTITION            MIN_MEMORY          \n",
      "5234135    256   1     R          dask-worker          0:13       2023-05-18T16:04:06  (null)               compute              10G                 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.squeue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 16:04:44 :: slurm :: INFO     -> Cancelling the job with ID: 5234135\n"
     ]
    }
   ],
   "source": [
    "slurm.scancel(Job_ID=5234135,loglevel=\"INFO\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `exclusive` argument DOES NOT automatically provide us the maximum available memory, number of cores, and walltime!\n",
    "It only provide you the exclusive usage of the node, meaning that no other job can run at the same time on the same node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 16:04:56 :: create_folder :: WARNING  -> Folder ./slurm already exists\n",
      "2023-05-18 16:04:56 :: create_folder :: WARNING  -> Folder ./slurm/logs already exists\n",
      "2023-05-18 16:04:56 :: create_folder :: WARNING  -> Folder ./slurm/output already exists\n",
      "/work/bb1153/b382289/mambaforge/envs/aqua/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 35239 instead\n",
      "  warnings.warn(\n",
      "2023-05-18 16:04:56 :: slurm :: WARNING  -> #!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p interactive\n",
      "#SBATCH -A bb1153\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=256\n",
      "#SBATCH --mem=466G\n",
      "#SBATCH -t 12:00:00\n",
      "#SBATCH --error=./slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=./slurm/output/dask-worker-%j.out\n",
      "#SBATCH --get-user-env\n",
      "#SBATCH --exclusive\n",
      "\n",
      "/work/bb1153/b382289/mambaforge/envs/aqua/bin/python -m distributed.cli.dask_worker tcp://136.172.124.5:38817 --nthreads 16 --nworkers 16 --memory-limit 29.10GiB --name dummy-name --nanny --death-timeout 60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "slurm.job(exclusive=True, cores=256, memory=\"500 GB\", queue = \"interactive\", walltime='12:00:00', jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID      CPUS  NODES ST         NAME                 TIME       START_TIME           DEPENDENCY           PARTITION            MIN_MEMORY          \n",
      "5234140    256   1     R          dask-worker          0:07       2023-05-18T16:04:57  (null)               interactive          466G                \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.squeue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.scancel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Creating the Job with maxumum availibale resources per the node"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function has an argument `max_resources_per_node`, which is `False` by default. If we set the argument to `True`, the number of cores, memory, and walltime will equal the maximum allowed by the selected queue/partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 16:05:28 :: create_folder :: WARNING  -> Folder ./slurm already exists\n",
      "2023-05-18 16:05:28 :: create_folder :: WARNING  -> Folder ./slurm/logs already exists\n",
      "2023-05-18 16:05:28 :: create_folder :: WARNING  -> Folder ./slurm/output already exists\n",
      "/work/bb1153/b382289/mambaforge/envs/aqua/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 46847 instead\n",
      "  warnings.warn(\n",
      "2023-05-18 16:05:29 :: slurm :: WARNING  -> #!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p compute\n",
      "#SBATCH -A bb1153\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=256\n",
      "#SBATCH --mem=235G\n",
      "#SBATCH -t 8:00:00\n",
      "#SBATCH --error=./slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=./slurm/output/dask-worker-%j.out\n",
      "\n",
      "/work/bb1153/b382289/mambaforge/envs/aqua/bin/python -m distributed.cli.dask_worker tcp://136.172.124.5:33263 --nthreads 16 --nworkers 16 --memory-limit 14.63GiB --name dummy-name --nanny --death-timeout 60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "slurm.job(max_resources=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOBID      CPUS  NODES ST         NAME                 TIME       START_TIME           DEPENDENCY           PARTITION            MIN_MEMORY          \n",
      "5234147    256   1     R          dask-worker          0:02       2023-05-18T16:05:35  (null)               compute              235G                \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.squeue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.scancel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the argument `max_resources_per_node=True`, the function `max_resources_per_node()` automatically extracts the following information about the resources in any queue:\n",
    "\n",
    " - Size of memory per node in Gigabytes: `max_memory`, \n",
    "\n",
    " - Maximum time for any job in the format \"days-hours:minutes:seconds: `max_walltime`\n",
    "\n",
    " - Number of CPUs per node: `max_cpus`, \n",
    "\n",
    " - Number of sockets per node: `max_sockets`\n",
    "\n",
    " - Number of cores per socket: `max_cores`, \n",
    " \n",
    " - Number of threads per core: `max_threads`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function can be also used alone to have info about a specific queue/partition.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('251.3671875 GB', '8:00:00', '256', '8', '16', '2')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.max_resources_per_node('compute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('502.9296875 GB', '12:00:00', '256', '8', '16', '2')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.max_resources_per_node('interactive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Redirecting the SLURM output to `/any/path/you/want`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slurm Job writes by default\n",
    "    - the errors into `./slurm/logs` directory \n",
    "    - the output into `./slurm/output/` directory\n",
    "\n",
    "If folders `/slurm`,  `/slurm/output`,  `/slurm/logs` do not exist, the function will create them automatically. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user can specify a custom path to redirect the SLURM outputs with the `path_to_output` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.job(path_to_output='/any/path/you/want')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating and Submitting the Job to the SLURM queue on Lumi (Under development)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to change the account name if you want to create the Slurm Job on Lumi with the function `job()`. Currently on levante, it  is `account=\"bb1153\"` and it is valid for the whole project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.job(account='Your_Lumi_account_name')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Canceling the Slurm Job"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Cancelling all jobs of user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.scancel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Canceling specific job"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Knowing the Job_ID, you can cancel your Job in the queue. For exaple, you can find your Job_ID using the function `squeue().` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_ID = 4929434\n",
    "slurm.scancel(Job_ID)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the status of Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.squeue()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tropical-rainfall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
