{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of slurm job creation and submission (on levante)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "0. [Install the following:]()\n",
    "\n",
    "    - [dask]() (conda)\n",
    "\n",
    "    - [dask_jobqueue]() (pip)\n",
    "\n",
    "    - [os, re]()\n",
    "\n",
    "1. [Importing the job module](#1-importing-the-job-module)\n",
    "\n",
    "2. [Creating the Slurm Job on LEVANTE](#2-creating-the-slurm-job-on-levante)\n",
    "\n",
    "    2.1 [Creating the Job with manual selection of cores, memory, and walltime](#21-creating-the-job-with-manual-selection-of-cores-memory-and-walltime)\n",
    "\n",
    "    2.2 [Creating the Job with exclusive node access](#22-creating-the-job-with-exclusive-node-access)\n",
    "\n",
    "    2.3 [Creating the Job with maxumum availibale resources per the node](#23-creating-the-job-with-maxumum-availibale-resources-per-the-node)\n",
    "\n",
    "    2.4 [Redirecting the SLURM output to /any/path/you/want](#24-redirecting-the-slurm-output-to-anypathyouwant)\n",
    "\n",
    "3. [Creating and Submitting the Job to the SLURM queue on Lumi](#3-creating-and-submitting-the-job-to-the-slurm-queue-on-lumi)\n",
    "    \n",
    "4. [Canceling the Slurm Job](#4-canceling-the-slurm-job)\n",
    "\n",
    "    3.1 [Canceling all jobs of the user](#41-cancelling-all-jobs-of-user)\n",
    "\n",
    "    3.2 [Canceling the specific Job](#42-canceling-specific-job)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the job module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The `job` module contains the following functions: `squeue`, `job`, `max_resources_per_node`, `scancel`, which allows us to create and operate the Slurm Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aqua.slurm import slurm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the Slurm Job on LEVANTE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating the Job with manual selection of cores, memory, and walltime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `slurm.job()` allows to create a job in the slurm queue on levante. Some options are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p compute\n",
      "#SBATCH -A bb1153\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=1\n",
      "#SBATCH --mem=10G\n",
      "#SBATCH -t 02:30:00\n",
      "#SBATCH --error=./slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=./slurm/output/dask-worker-%j.out\n",
      "\n",
      "/work/bb1153/b382289/mambaforge/envs/aqua/bin/python -m distributed.cli.dask_worker tcp://136.172.124.5:33807 --nthreads 1 --memory-limit 9.31GiB --name dummy-name --nanny --death-timeout 60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/bb1153/b382289/mambaforge/envs/aqua/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 32937 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "slurm.job()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the status of created Job in the queue using the function `squeue()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           5233510   compute dask-wor  b382289 PD       0:00      1 (Priority)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.squeue()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the Job has the following attributes:\n",
    "\n",
    "- `exclusive=False`: If True, the job will be submitted asking for exclusive access to the node.\n",
    "- `max_resources=False`: If True, the job will be submitted asking for the maximum resources available on the node.\n",
    "- `cores=1`: number of cores per socket.\n",
    "- `memory=\"10 GB\"`: real memory required per node.\n",
    "- `queue=\"compute\"`: queue/partition to which SLURM submit the job.\n",
    "- `walltime=\"02:30:00\"`: duration of the allocation.\n",
    "- `jobs=1`: factor of assignment scaling across multiple nodes.\n",
    "- `account=bb1153`: account that submits the job. It is the project id on levante.\n",
    "- `path_to_output=\".\"`: path where log, err and output files are stored.\n",
    "\n",
    " \n",
    "If you want to use a different amount of cores, memory, wall time, jobs, or a different queue, you can specify it as an argument of function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p interactive\n",
      "#SBATCH -A bb1153\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=8\n",
      "#SBATCH --mem=47G\n",
      "#SBATCH -t 00:30:00\n",
      "#SBATCH --error=./slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=./slurm/output/dask-worker-%j.out\n",
      "\n",
      "/work/bb1153/b382289/mambaforge/envs/aqua/bin/python -m distributed.cli.dask_worker tcp://136.172.124.5:40169 --nthreads 2 --nworkers 4 --memory-limit 11.64GiB --name dummy-name --nanny --death-timeout 60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/bb1153/b382289/mambaforge/envs/aqua/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 40665 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "slurm.job(cores=8, memory=\"50 GB\", queue=\"interactive\", walltime='00:30:00', jobs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Creating the Job with exclusive node access"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The function has an argument `exclusive`, which is False by default.  If we set the argument to True, we will get exclusive access to the node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/bb1153/b382289/mambaforge/envs/aqua/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 45309 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p compute\n",
      "#SBATCH -A bb1153\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=1\n",
      "#SBATCH --mem=10G\n",
      "#SBATCH -t 02:30:00\n",
      "#SBATCH --error=./slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=./slurm/output/dask-worker-%j.out\n",
      "#SBATCH --get-user-env\n",
      "#SBATCH --exclusive\n",
      "\n",
      "/work/bb1153/b382289/mambaforge/envs/aqua/bin/python -m distributed.cli.dask_worker tcp://136.172.124.5:34597 --nthreads 1 --memory-limit 9.31GiB --name dummy-name --nanny --death-timeout 60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "slurm.job(exclusive=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `exclusive` argument DOES NOT automatically provide us the maximum available memory, number of cores, and walltime!\n",
    "It only provide you the exclusive usage of the node, meaning that no other job can run at the same time on the same node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p interactive\n",
      "#SBATCH -A bb1153\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=256\n",
      "#SBATCH --mem=466G\n",
      "#SBATCH -t 12:00:00\n",
      "#SBATCH --error=./slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=./slurm/output/dask-worker-%j.out\n",
      "#SBATCH --get-user-env\n",
      "#SBATCH --exclusive\n",
      "\n",
      "/work/bb1153/b382289/mambaforge/envs/aqua/bin/python -m distributed.cli.dask_worker tcp://136.172.124.5:42651 --nthreads 16 --nworkers 16 --memory-limit 29.10GiB --name dummy-name --nanny --death-timeout 60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/bb1153/b382289/mambaforge/envs/aqua/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 36309 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "slurm.job(exclusive=True, cores=256, memory=\"500 GB\", queue = \"interactive\", walltime='12:00:00', jobs=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Creating the Job with maxumum availibale resources per the node"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function has an argument `max_resources_per_node`, which is `False` by default. If we set the argument to `True`, the number of cores, memory, and walltime will equal the maximum allowed by the selected queue/partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p compute\n",
      "#SBATCH -A bb1153\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=256\n",
      "#SBATCH --mem=235G\n",
      "#SBATCH -t 8:00:00\n",
      "#SBATCH --error=./slurm/logs/dask-worker-%j.err\n",
      "#SBATCH --output=./slurm/output/dask-worker-%j.out\n",
      "\n",
      "/work/bb1153/b382289/mambaforge/envs/aqua/bin/python -m distributed.cli.dask_worker tcp://136.172.124.5:38707 --nthreads 16 --nworkers 16 --memory-limit 14.63GiB --name dummy-name --nanny --death-timeout 60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/bb1153/b382289/mambaforge/envs/aqua/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 46233 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "slurm.job(max_resources=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the argument `max_resources_per_node=True`, the function `max_resources_per_node()` automatically extracts the following information about the resources in any queue:\n",
    "\n",
    " - Size of memory per node in Gigabytes: `max_memory`, \n",
    "\n",
    " - Maximum time for any job in the format \"days-hours:minutes:seconds: `max_walltime`\n",
    "\n",
    " - Number of CPUs per node: `max_cpus`, \n",
    "\n",
    " - Number of sockets per node: `max_sockets`\n",
    "\n",
    " - Number of cores per socket: `max_cores`, \n",
    " \n",
    " - Number of threads per core: `max_threads`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function can be also used alone to have info about a specific queue/partition.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('251.3671875 GB', '8:00:00', '256', '8', '16', '2')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.max_resources_per_node('compute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('502.9296875 GB', '12:00:00', '256', '8', '16', '2')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.max_resources_per_node('interactive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Redirecting the SLURM output to `/any/path/you/want`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slurm Job writes by default\n",
    "    - the errors into `./slurm/logs` directory \n",
    "    - the output into `./slurm/output/` directory\n",
    "\n",
    "If folders `/slurm`,  `/slurm/output`,  `/slurm/logs` do not exist, the function will create them automatically. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user can specify a custom path to redirect the SLURM outputs with the `path_to_output` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.job(path_to_output='/any/path/you/want')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating and Submitting the Job to the SLURM queue on Lumi (Under development)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to change the account name if you want to create the Slurm Job on Lumi with the function `job()`. Currently on levante, it  is `account=\"bb1153\"` and it is valid for the whole project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.job(account='Your_Lumi_account_name')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Canceling the Slurm Job"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Cancelling all jobs of user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm.scancel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Canceling specific job"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Knowing the Job_ID, you can cancel your Job in the queue. For exaple, you can find your Job_ID using the function `squeue().` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_ID = 4929434\n",
    "slurm.scancel(Job_ID)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the status of canceled Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.squeue()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tropical-rainfall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
